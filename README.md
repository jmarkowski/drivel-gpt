# NanoGPT

A nano-version of a Generatively Pre-trained Transformer.

The purpose of the course is train a transfomer-based model.

## Purpose

To understand and appreciate how chatGPT works under the hood.


## Training the Transformer

Use the [nanoGPT](https://github.com/karpathy/nanoGPT.git) repository for
training the transformer on any given text. It's a simple implementation.

Training produces neural-network weights for the transformer model.


1. Define a transformer and all of its pieces.
2. Train it on the dataset.

# Resources

* [Attention is All You Need](https://arxiv.org/abs/1706.03762)
* [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)
* [GPT Visual Guide](https://twitter.com/akshay_pachaar/status/1647940492712345601)
* [Neural Networks: Zero to Hero](https://karpathy.ai/zero-to-hero.html)
